{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of fraudulent transactions:  0.001727485630620034\n",
      "Epoch: 0 Current loss: 1.3912 Elapsed time: 0.75 seconds\n",
      "Current accuracy: 0.17%\n",
      "Epoch: 10 Current loss: 1.3890 Elapsed time: 0.63 seconds\n",
      "Current accuracy: 2.36%\n",
      "Epoch: 20 Current loss: 1.3599 Elapsed time: 0.67 seconds\n",
      "Current accuracy: 37.11%\n",
      "Epoch: 30 Current loss: 1.2543 Elapsed time: 0.65 seconds\n",
      "Current accuracy: 91.55%\n",
      "Epoch: 40 Current loss: 1.0893 Elapsed time: 0.64 seconds\n",
      "Current accuracy: 96.71%\n",
      "Epoch: 50 Current loss: 0.9628 Elapsed time: 0.62 seconds\n",
      "Current accuracy: 98.85%\n",
      "Epoch: 60 Current loss: 0.8971 Elapsed time: 0.63 seconds\n",
      "Current accuracy: 99.60%\n",
      "Epoch: 70 Current loss: 0.8693 Elapsed time: 0.63 seconds\n",
      "Current accuracy: 99.55%\n",
      "Epoch: 80 Current loss: 0.8498 Elapsed time: 0.63 seconds\n",
      "Current accuracy: 99.62%\n",
      "Epoch: 90 Current loss: 0.8382 Elapsed time: 0.62 seconds\n",
      "Current accuracy: 99.38%\n",
      "Final accuracy: 99.55%\n",
      "Final fraud specific accuracy: 82.11%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import and store dataset\n",
    "credit_card_data = pd.read_csv('creditcard.csv')\n",
    "#print(credit_card_data)\n",
    "\n",
    "# Splitting data into 4 sets\n",
    "# 1. Shuffle/randomize data\n",
    "# 2. One-hot encoding\n",
    "# 3. Normalize\n",
    "# 4. Splitting up X/y values\n",
    "# 5. Convert data_frames to numpy arrays (float32)\n",
    "# 6. Splitting the final data into X/y train/test\n",
    "\n",
    "# Shuffle and randomize data\n",
    "shuffled_data = credit_card_data.sample(frac=1)\n",
    "# Change Class column into Class_0 ([1 0] for legit data) and Class_1 ([0 1] for fraudulent data)\n",
    "one_hot_data = pd.get_dummies(shuffled_data, columns=['Class'])\n",
    "# Change all values into numbers between 0 and 1\n",
    "normalized_data = (one_hot_data - one_hot_data.min()) / (one_hot_data.max() - one_hot_data.min())\n",
    "# Store just columns V1 through V28 in df_X and columns Class_0 and Class_1 in df_y\n",
    "df_X = normalized_data.drop(['Class_0', 'Class_1'], axis=1)\n",
    "df_y = normalized_data[['Class_0', 'Class_1']]\n",
    "# Convert both data_frames into np arrays of float32\n",
    "ar_X, ar_y = np.asarray(df_X.values, dtype='float32'), np.asarray(df_y.values, dtype='float32')\n",
    "# Allocate first 80% of data into training data and remaining 20% into testing data\n",
    "train_size = int(0.8 * len(ar_X))\n",
    "(raw_X_train, raw_y_train) = (ar_X[:train_size], ar_y[:train_size])\n",
    "(raw_X_test, raw_y_test) = (ar_X[train_size:], ar_y[train_size:])\n",
    "\n",
    "# Gets a percent of fraud vs legit transactions (0.0017% of transactions are fraudulent)\n",
    "count_legit, count_fraud = np.unique(credit_card_data['Class'], return_counts=True)[1]\n",
    "fraud_ratio = float(count_fraud / (count_legit + count_fraud))\n",
    "print('Percent of fraudulent transactions: ', fraud_ratio)\n",
    "\n",
    "# Applies a logit weighting of 578 (1/0.0017) to fraudulent transactions to cause model to pay more attention to them\n",
    "weighting = 1 / fraud_ratio\n",
    "raw_y_train[:, 1] = raw_y_train[:, 1] * weighting\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 30 cells for the input\n",
    "input_dimensions = ar_X.shape[1]\n",
    "# 2 cells for the output\n",
    "output_dimensions = ar_y.shape[1]\n",
    "# 100 cells for the 1st layer\n",
    "num_layer_1_cells = 100\n",
    "# 150 cells for the second layer\n",
    "num_layer_2_cells = 150\n",
    "\n",
    "# We will use these as inputs to the model when it comes time to train it (assign values at run time)\n",
    "X_train_node = tf.placeholder(tf.float32, [None, input_dimensions], name='X_train')\n",
    "y_train_node = tf.placeholder(tf.float32, [None, output_dimensions], name='y_train')\n",
    "\n",
    "# We will use these as inputs to the model once it comes time to test it\n",
    "X_test_node = tf.constant(raw_X_test, name='X_test')\n",
    "y_test_node = tf.constant(raw_y_test, name='y_test')\n",
    "\n",
    "# First layer takes in input and passes output to 2nd layer\n",
    "weight_1_node = tf.Variable(tf.zeros([input_dimensions, num_layer_1_cells]), name='weight_1')\n",
    "biases_1_node = tf.Variable(tf.zeros([num_layer_1_cells]), name='biases_1')\n",
    "\n",
    "# Second layer takes in input from 1st layer and passes output to 3rd layer\n",
    "weight_2_node = tf.Variable(tf.zeros([num_layer_1_cells, num_layer_2_cells]), name='weight_2')\n",
    "biases_2_node = tf.Variable(tf.zeros([num_layer_2_cells]), name='biases_2')\n",
    "\n",
    "# Third layer takes in input from 2nd layer and outputs [1 0] or [0 1] depending on fraud vs legit\n",
    "weight_3_node = tf.Variable(tf.zeros([num_layer_2_cells, output_dimensions]), name='weight_3')\n",
    "biases_3_node = tf.Variable(tf.zeros([output_dimensions]), name='biases_3')\n",
    "\n",
    "\n",
    "# Function to run an input tensor through the 3 layers and output a tensor that will give us a fraud/legit result\n",
    "# Each layer uses a different function to fit lines through the data and predict whether a given input tensor will \\\n",
    "#   result in a fraudulent or legitimate transaction\n",
    "def network(input_tensor):\n",
    "    # Sigmoid fits modified data well\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(input_tensor, weight_1_node) + biases_1_node)\n",
    "    # Dropout prevents model from becoming lazy and over confident\n",
    "    layer2 = tf.nn.dropout(tf.nn.sigmoid(tf.matmul(layer1, weight_2_node) + biases_2_node), 0.85)\n",
    "    # Softmax works very well with one hot encoding which is how results are outputted\n",
    "    layer3 = tf.nn.softmax(tf.matmul(layer2, weight_3_node) + biases_3_node)\n",
    "    return layer3\n",
    "\n",
    "\n",
    "# Used to predict what results will be given training or testing input data\n",
    "# Remember, X_train_node is just a placeholder for now. We will enter values at run time\n",
    "y_train_prediction = network(X_train_node)\n",
    "y_test_prediction = network(X_test_node)\n",
    "\n",
    "# Cross entropy loss function measures differences between actual output and predicted output\n",
    "cross_entropy = tf.losses.softmax_cross_entropy(y_train_node, y_train_prediction)\n",
    "\n",
    "# Adam optimizer function will try to minimize loss (cross_entropy) but changing the 3 layers' variable values at a\n",
    "#   learning rate of 0.005\n",
    "optimizer = tf.train.AdamOptimizer(0.005).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of the actual result vs the predicted result\n",
    "def calculate_accuracy(actual, predicted):\n",
    "    actual = np.argmax(actual, 1)\n",
    "    predicted = np.argmax(predicted, 1)\n",
    "    return (100 * np.sum(np.equal(predicted, actual)) / predicted.shape[0])\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "import time\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        _, cross_entropy_score = session.run([optimizer, cross_entropy],\n",
    "                                             feed_dict={X_train_node: raw_X_train, y_train_node: raw_y_train})\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            timer = time.time() - start_time\n",
    "\n",
    "            print('Epoch: {}'.format(epoch), 'Current loss: {0:.4f}'.format(cross_entropy_score),\n",
    "                  'Elapsed time: {0:.2f} seconds'.format(timer))\n",
    "\n",
    "            final_y_test = y_test_node.eval()\n",
    "            final_y_test_prediction = y_test_prediction.eval()\n",
    "            final_accuracy = calculate_accuracy(final_y_test, final_y_test_prediction)\n",
    "            print(\"Current accuracy: {0:.2f}%\".format(final_accuracy))\n",
    "\n",
    "    final_y_test = y_test_node.eval()\n",
    "    final_y_test_prediction = y_test_prediction.eval()\n",
    "    final_accuracy = calculate_accuracy(final_y_test, final_y_test_prediction)\n",
    "    print(\"Final accuracy: {0:.2f}%\".format(final_accuracy))\n",
    "\n",
    "final_fraud_y_test = final_y_test[final_y_test[:, 1] == 1]\n",
    "final_fraud_y_test_prediction = final_y_test_prediction[final_y_test[:, 1] == 1]\n",
    "final_fraud_accuracy = calculate_accuracy(final_fraud_y_test, final_fraud_y_test_prediction)\n",
    "print('Final fraud specific accuracy: {0:.2f}%'.format(final_fraud_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
